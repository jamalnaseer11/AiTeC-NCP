{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8652161,"sourceType":"datasetVersion","datasetId":5182680}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np         #numpy is used for most of the mathematical (as np ables us to use it as a shortcut)\nimport matplotlib.pyplot as plt     # for charts and graphs\nimport pandas as pd\n\n\n\n\ndataset = pd.read_csv('/kaggle/input/50-startups/50_Startups.csv')       #stores all the data and columns into the array dataset\nX= dataset.iloc[:, :-1].values        # stores all the values of the columns except the last one (because last one is dependant )\ny= dataset.iloc[:, 4].values            # stores all the values of the last one in y (dependant)\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder \nlabelencoder_X= LabelEncoder()      #Labelencoder does simple encoding which gives values to the categories which is sometime not needed    // Onehotencoder      labelencoder_X = LabelEncoder()                                       #makes separate columns       \nX[:, 3]= labelencoder_X.fit_transform(X[:, 3])                         # fits label encoding into first columns and then transforms it\n\n\nfirst_column = X[:, 3].reshape(-1, 1)\nonehotencoder =OneHotEncoder()    \n# inside brackets specifies that first column needs to be labeled\nefc= onehotencoder.fit_transform(first_column).toarray()\nX = np.hstack(( efc, X[:, 0:3]))\n\n\n#to avoid the dummy variable trap, we remove the first column\nX= X[:, 1:]\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test , y_train , y_test = train_test_split(X, y , test_size= 0.2, random_state=0)     \n\n\n#fitting the regressor\nfrom sklearn.linear_model import LinearRegression\nregressor= LinearRegression()\nregressor.fit(X_train,y_train)\n\ny_pred= regressor.predict(X_test)\n\n\n\n#building the optimal solution by backward elimination\nimport statsmodels.api as sm\n\n\n# add a column of 1s so that in the equation the constant has an x0 \nX=np.append(arr =np.ones((50,1)).astype(int) , values= X, axis=1)\n#starting backward elimination\nX=X.astype(float)\nX_opt= X[:,[0,1,2,3,4,5]]          #X_opt will contain only the optiaml independent variables after backward elimination, initailly the X_opt contains all the variable but will be removed one by one thru backward elimination\nregressor_ols= sm.OLS(endog=y, exog= X_opt).fit()\nregressor_ols.summary()\n\nX_opt= X[:,[0,1,3,4,5]]          #X_opt will contain only the optiaml independent variables after backward elimination, initailly the X_opt contains all the variable but will be removed one by one thru backward elimination\nregressor_ols= sm.OLS(endog=y, exog= X_opt).fit()\nregressor_ols.summary()\n\n\n\n\nX_opt= X[:,[0,3,4,5]]          #X_opt will contain only the optiaml independent variables after backward elimination, initailly the X_opt contains all the variable but will be removed one by one thru backward elimination\nregressor_ols= sm.OLS(endog=y, exog= X_opt).fit()\nregressor_ols.summary()         #check for o values and if the highest p vaue is greater then decided SL then remove that index\n\nX_opt= X[:,[0,3,5]]          #X_opt will contain only the optiaml independent variables after backward elimination, initailly the X_opt contains all the variable but will be removed one by one thru backward elimination\nregressor_ols= sm.OLS(endog=y, exog= X_opt).fit()\nregressor_ols.summary()\n\n\nX_opt= X[:,[0,3]]          #X_opt will contain only the optiaml independent variables after backward elimination, initailly the X_opt contains all the variable but will be removed one by one thru backward elimination\nregressor_ols= sm.OLS(endog=y, exog= X_opt).fit()\nregressor_ols.summary()\n\n\n#the most optimal model has been created and can now be used to make better predictions\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-10T05:13:11.703112Z","iopub.execute_input":"2024-06-10T05:13:11.703974Z","iopub.status.idle":"2024-06-10T05:13:11.789158Z","shell.execute_reply.started":"2024-06-10T05:13:11.703936Z","shell.execute_reply":"2024-06-10T05:13:11.787813Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.947\nModel:                            OLS   Adj. R-squared:                  0.945\nMethod:                 Least Squares   F-statistic:                     849.8\nDate:                Mon, 10 Jun 2024   Prob (F-statistic):           3.50e-32\nTime:                        05:13:11   Log-Likelihood:                -527.44\nNo. Observations:                  50   AIC:                             1059.\nDf Residuals:                      48   BIC:                             1063.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\nx1             0.8543      0.029     29.151      0.000       0.795       0.913\n==============================================================================\nOmnibus:                       13.727   Durbin-Watson:                   1.116\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\nSkew:                          -0.911   Prob(JB):                     9.44e-05\nKurtosis:                       5.361   Cond. No.                     1.65e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.65e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"","text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.947</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   849.8</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 10 Jun 2024</td> <th>  Prob (F-statistic):</th> <td>3.50e-32</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>05:13:11</td>     <th>  Log-Likelihood:    </th> <td> -527.44</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    48</td>      <th>  BIC:               </th> <td>   1063.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 4.903e+04</td> <td> 2537.897</td> <td>   19.320</td> <td> 0.000</td> <td> 4.39e+04</td> <td> 5.41e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    0.8543</td> <td>    0.029</td> <td>   29.151</td> <td> 0.000</td> <td>    0.795</td> <td>    0.913</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>13.727</td> <th>  Durbin-Watson:     </th> <td>   1.116</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  18.536</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.911</td> <th>  Prob(JB):          </th> <td>9.44e-05</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.361</td> <th>  Cond. No.          </th> <td>1.65e+05</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.65e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems.","text/latex":"\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.947   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.945   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     849.8   \\\\\n\\textbf{Date:}             & Mon, 10 Jun 2024 & \\textbf{  Prob (F-statistic):} &  3.50e-32   \\\\\n\\textbf{Time:}             &     05:13:11     & \\textbf{  Log-Likelihood:    } &   -527.44   \\\\\n\\textbf{No. Observations:} &          50      & \\textbf{  AIC:               } &     1059.   \\\\\n\\textbf{Df Residuals:}     &          48      & \\textbf{  BIC:               } &     1063.   \\\\\n\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{const} &    4.903e+04  &     2537.897     &    19.320  &         0.000        &     4.39e+04    &     5.41e+04     \\\\\n\\textbf{x1}    &       0.8543  &        0.029     &    29.151  &         0.000        &        0.795    &        0.913     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 13.727 & \\textbf{  Durbin-Watson:     } &    1.116  \\\\\n\\textbf{Prob(Omnibus):} &  0.001 & \\textbf{  Jarque-Bera (JB):  } &   18.536  \\\\\n\\textbf{Skew:}          & -0.911 & \\textbf{  Prob(JB):          } & 9.44e-05  \\\\\n\\textbf{Kurtosis:}      &  5.361 & \\textbf{  Cond. No.          } & 1.65e+05  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n [2] The condition number is large, 1.65e+05. This might indicate that there are \\newline\n strong multicollinearity or other numerical problems."},"metadata":{}}]}]}